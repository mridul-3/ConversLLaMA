{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U transformers peft accelerate optimum\n!pip install datasets==2.15.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-28T10:23:01.138207Z","iopub.execute_input":"2024-04-28T10:23:01.138606Z","iopub.status.idle":"2024-04-28T10:23:43.401881Z","shell.execute_reply.started":"2024-04-28T10:23:01.138574Z","shell.execute_reply":"2024-04-28T10:23:43.400744Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting datasets==2.15.0\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.6)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.15.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (2024.2.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.15.0)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, dill, multiprocess, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.2.0\n    Uninstalling fsspec-2024.2.0:\n      Successfully uninstalled fsspec-2024.2.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.18.0\n    Uninstalling datasets-2.18.0:\n      Successfully uninstalled datasets-2.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngcsfs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ns3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.15.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install auto-gptq","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:23:43.403891Z","iopub.execute_input":"2024-04-28T10:23:43.404219Z","iopub.status.idle":"2024-04-28T10:23:58.636818Z","shell.execute_reply.started":"2024-04-28T10:23:43.404188Z","shell.execute_reply":"2024-04-28T10:23:58.635659Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting auto-gptq\n  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.29.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.15.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq)\n  Downloading gekko-1.1.1-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.1.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.4.3)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.40.1)\nRequirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.10.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.66.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2023.10.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.19.1)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.6)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.15)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.9.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0->auto-gptq) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\nDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gekko-1.1.1-py3-none-any.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge, gekko, auto-gptq\nSuccessfully installed auto-gptq-0.7.1 gekko-1.1.1 rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:24:04.936560Z","iopub.execute_input":"2024-04-28T10:24:04.937225Z","iopub.status.idle":"2024-04-28T10:24:05.213802Z","shell.execute_reply.started":"2024-04-28T10:24:04.937190Z","shell.execute_reply":"2024-04-28T10:24:05.212824Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd66e9eba35e4ed5a52d7da4b5346d73"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, GPTQConfig\nimport torch\n\nMODEL_ID = \"mridul161203/Meta-llama-3-8B-GPTQ\"\n\nconfig = AutoConfig.from_pretrained(MODEL_ID)\n#config.quantization_config[\"use_exllama\"] = True\nconfig.quantization_config[\"disable_exllama\"] = False\nconfig.quantization_config[\"exllama_config\"] = {\"version\":2}\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", config=config)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:24:21.160358Z","iopub.execute_input":"2024-04-28T10:24:21.161468Z","iopub.status.idle":"2024-04-28T10:25:15.540354Z","shell.execute_reply.started":"2024-04-28T10:24:21.161433Z","shell.execute_reply":"2024-04-28T10:25:15.539435Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10c0f1a2bfb400bb71cfb8f84367acc"}},"metadata":{}},{"name":"stderr","text":"Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n2024-04-28 10:24:28.996566: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 10:24:28.996722: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 10:24:29.131461: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/5.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e942eed6d364ca4bc92caf5870341e3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\nThe cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\nThe sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.quantization_config.to_dict()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:25:35.679030Z","iopub.execute_input":"2024-04-28T10:25:35.680415Z","iopub.status.idle":"2024-04-28T10:25:35.689015Z","shell.execute_reply.started":"2024-04-28T10:25:35.680379Z","shell.execute_reply":"2024-04-28T10:25:35.687754Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'quant_method': <QuantizationMethod.GPTQ: 'gptq'>,\n 'bits': 4,\n 'tokenizer': None,\n 'dataset': None,\n 'group_size': 128,\n 'damp_percent': 0.01,\n 'desc_act': True,\n 'sym': True,\n 'true_sequential': True,\n 'use_cuda_fp16': False,\n 'model_seqlen': None,\n 'block_name_to_quantize': None,\n 'module_name_preceding_first_block': None,\n 'batch_size': 1,\n 'pad_token_id': None,\n 'use_exllama': True,\n 'max_input_length': None,\n 'exllama_config': {'version': 2},\n 'cache_block_outputs': True,\n 'modules_in_block_to_quantize': None}"},"metadata":{}}]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:25:38.940114Z","iopub.execute_input":"2024-04-28T10:25:38.940816Z","iopub.status.idle":"2024-04-28T10:25:38.951219Z","shell.execute_reply.started":"2024-04-28T10:25:38.940785Z","shell.execute_reply":"2024-04-28T10:25:38.950225Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:25:44.268294Z","iopub.execute_input":"2024-04-28T10:25:44.268672Z","iopub.status.idle":"2024-04-28T10:25:44.760065Z","shell.execute_reply.started":"2024-04-28T10:25:44.268644Z","shell.execute_reply":"2024-04-28T10:25:44.759125Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"trainable params: 6,815,744 || all params: 1,057,755,136 || trainable%: 0.6443593387572074\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, GPTQConfig\n\nMODEL_ID = \"mridul161203/Meta-llama-3-8B-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T07:20:20.819894Z","iopub.execute_input":"2024-05-09T07:20:20.820303Z","iopub.status.idle":"2024-05-09T07:20:28.884372Z","shell.execute_reply.started":"2024-05-09T07:20:20.820269Z","shell.execute_reply":"2024-05-09T07:20:28.883465Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd594fcf62e45a1845c1e25ed7b67d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29256702a920474c9aae9bb350f1b5b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46a62a6c973483d94b3aca0024a5e93"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset(\"knkarthick/dialogsum\")\nprint(data)\ndata = data.map(lambda samples: tokenizer(samples[\"dialogue\"]), batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T07:28:27.767074Z","iopub.execute_input":"2024-05-09T07:28:27.767477Z","iopub.status.idle":"2024-05-09T07:28:29.486858Z","shell.execute_reply.started":"2024-05-09T07:28:27.767432Z","shell.execute_reply":"2024-05-09T07:28:29.485807Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(data)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T07:28:46.650959Z","iopub.execute_input":"2024-05-09T07:28:46.651820Z","iopub.status.idle":"2024-05-09T07:28:46.657099Z","shell.execute_reply.started":"2024-05-09T07:28:46.651778Z","shell.execute_reply":"2024-05-09T07:28:46.656137Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'attention_mask'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'attention_mask'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'attention_mask'],\n        num_rows: 1500\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade torch","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:26:13.478425Z","iopub.execute_input":"2024-04-28T10:26:13.478808Z","iopub.status.idle":"2024-04-28T10:28:28.644567Z","shell.execute_reply.started":"2024-04-28T10:26:13.478777Z","shell.execute_reply":"2024-04-28T10:28:28.643086Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting torch\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch)\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 triton-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:28:36.063039Z","iopub.execute_input":"2024-04-28T10:28:36.063753Z","iopub.status.idle":"2024-04-28T10:28:36.067949Z","shell.execute_reply.started":"2024-04-28T10:28:36.063720Z","shell.execute_reply":"2024-04-28T10:28:36.066953Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\n# needed for llama 2 tokenizer\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=data[\"train\"],\n    args=TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=10,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"adamw_hf\"\n    ),\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:28:39.654100Z","iopub.execute_input":"2024-04-28T10:28:39.654944Z","iopub.status.idle":"2024-04-28T10:31:14.394434Z","shell.execute_reply.started":"2024-04-28T10:28:39.654914Z","shell.execute_reply":"2024-04-28T10:31:14.393473Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240428_102907-4pmt49m3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gaurav-org/huggingface/runs/4pmt49m3' target=\"_blank\">lilac-puddle-5</a></strong> to <a href='https://wandb.ai/gaurav-org/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gaurav-org/huggingface' target=\"_blank\">https://wandb.ai/gaurav-org/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gaurav-org/huggingface/runs/4pmt49m3' target=\"_blank\">https://wandb.ai/gaurav-org/huggingface/runs/4pmt49m3</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  the RNG state during each checkpoint. Note that under torch.compile,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 01:38, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.194400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.139600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.239500</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.156200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.175600</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.937700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.938200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.967000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.035700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.078700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=2.0862542033195495, metrics={'train_runtime': 152.8942, 'train_samples_per_second': 0.262, 'train_steps_per_second': 0.065, 'total_flos': 26805144821760.0, 'train_loss': 2.0862542033195495, 'epoch': 0.0032102728731942215})"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(MODEL_ID)\ntokenizer.push_to_hub(MODEL_ID)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:31:21.337725Z","iopub.execute_input":"2024-04-28T10:31:21.338411Z","iopub.status.idle":"2024-04-28T10:31:25.604603Z","shell.execute_reply.started":"2024-04-28T10:31:21.338377Z","shell.execute_reply":"2024-04-28T10:31:25.603472Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3fd4c16fc8c44daba42145f8812c770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2c399ad91aa4dc49669e938ab7af4a3"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/gaurav021201/Meta-Llama-3-8B-GPTQ/commit/47f769d4c4ec3c2b06e837b42a42a215139a5e6f', commit_message='Upload tokenizer', commit_description='', oid='47f769d4c4ec3c2b06e837b42a42a215139a5e6f', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel\n\nmodel = PeftModel.from_pretrained(model, MODEL_ID)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:31:47.238083Z","iopub.execute_input":"2024-04-28T10:31:47.238451Z","iopub.status.idle":"2024-04-28T10:31:48.175511Z","shell.execute_reply.started":"2024-04-28T10:31:47.238424Z","shell.execute_reply":"2024-04-28T10:31:48.174457Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2645efad339043609ff6a77e24c2ae17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b68cf50cd247faba6bbe244be5753f"}},"metadata":{}}]},{"cell_type":"code","source":"merged_model = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:32:20.393043Z","iopub.execute_input":"2024-04-28T10:32:20.393548Z","iopub.status.idle":"2024-04-28T10:32:21.374300Z","shell.execute_reply.started":"2024-04-28T10:32:20.393511Z","shell.execute_reply":"2024-04-28T10:32:21.371980Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_and_unload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:784\u001b[0m, in \u001b[0;36mLoraModel.merge_and_unload\u001b[0;34m(self, progressbar, safe_merge, adapter_names)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_and_unload\u001b[39m(\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m, progressbar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, safe_merge: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, adapter_names: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m    759\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m    as a standalone model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unload_and_optionally_merge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_merge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_merge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_names\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:426\u001b[0m, in \u001b[0;36mLoraModel._unload_and_optionally_merge\u001b[0;34m(self, merge, progressbar, safe_merge, adapter_names)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unload_and_optionally_merge\u001b[39m(\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    420\u001b[0m     merge\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     adapter_names: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ):\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m merge:\n\u001b[0;32m--> 426\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_merge_allowed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     key_list \u001b[38;5;241m=\u001b[39m [key \u001b[38;5;28;01mfor\u001b[39;00m key, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnamed_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m key]\n\u001b[1;32m    429\u001b[0m     desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand merging \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merge \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:404\u001b[0m, in \u001b[0;36mLoraModel._check_merge_allowed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Verify that the configuration supports merging.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03mCurrently gptq quantization and replicated layers do not support merging.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptq\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot merge LORA layers when the model is gptq quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer_replication\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot merge LORA layers when base model layers are replicated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Cannot merge LORA layers when the model is gptq quantized"],"ename":"ValueError","evalue":"Cannot merge LORA layers when the model is gptq quantized","output_type":"error"}]},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a friendly chatbot assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, what are the keys things to keep in mind during a job interview ?\"},]\ngen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to('cuda')\n\ngen_output = model.generate(input_ids=gen_input,\n                            max_new_tokens=256,\n                            do_sample=True,\n                            temperature=0.7,\n                            top_k=50, \n                            top_p=0.95, \n                            repetition_penalty=1.1)\nout = tokenizer.decode(gen_output[0], skip_special_tokens=True)\nprint(out)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:34:55.217689Z","iopub.execute_input":"2024-04-28T10:34:55.218515Z","iopub.status.idle":"2024-04-28T10:39:32.498922Z","shell.execute_reply.started":"2024-04-28T10:34:55.218476Z","shell.execute_reply":"2024-04-28T10:39:32.497996Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"\nNo chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|im_start|>system\nYou are a friendly chatbot assistant.<|im_end|>\n<|im_start|>user\nHello, what are the keys things to keep in mind during a job interview?<|im_end|>\n<|im_start|>system\nWell, one of the key thing is to be confident and well-prepared. What else do you think is important? <|im_end|>\nThis is an example of how an interactive learning system can be used for knowledge elicitation: the user starts by asking questions about some topic, and as they ask more specific questions, the system responds with progressively more detailed answers. The system then learns from the conversation that it had with the user.\n\n## 2.2 Learning to Ask Questions\n\nWhile the first approach works quite nicely when the domain is small (a few dozen concepts) or when the user only has access to a single modality (e.g., text), we cannot expect this to work well if the user is provided with multiple modalities such as spoken language, images and videos.\nTo illustrate the problem, consider the following scenario:\n    - Alice wants to learn how to make different kinds of pasta.\n    - She opens up a virtual world where she sees herself in the kitchen.\n    - Her task is to learn how to make different types of pasta. There will be other users who will help her learn through interaction.\n    - Alice can see and interact with different objects on the table: bowls, pans, tools.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"text = 'Please write a code to concat two strings in python input_string1 = input() '\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\nout = model.generate(**inputs)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-28T10:32:51.020624Z","iopub.execute_input":"2024-04-28T10:32:51.021053Z","iopub.status.idle":"2024-04-28T10:32:53.443681Z","shell.execute_reply.started":"2024-04-28T10:32:51.021011Z","shell.execute_reply":"2024-04-28T10:32:53.442658Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Please write a code to concat two strings in python input_string1 = input()  input_string\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GenerationConfig\nfrom time import perf_counter\nfrom rich import print\n\ngeneration_config = GenerationConfig(\n    penalty_alpha=0.6, \n    do_sample = True, \n    top_k=10, \n    temperature=0.9, \n    repetition_penalty=1.2,\n    max_new_tokens=1200,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nstart_time = perf_counter()\noutputs = model.generate(**inputs, generation_config=generation_config)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nend_time = perf_counter()\noutput_time = end_time - start_time\nprint(f\"Time taken for inference: {round(output_time,2)} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T23:25:59.976584Z","iopub.execute_input":"2024-04-27T23:25:59.976991Z","iopub.status.idle":"2024-04-27T23:27:23.958437Z","shell.execute_reply.started":"2024-04-27T23:25:59.976954Z","shell.execute_reply":"2024-04-27T23:27:23.957610Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  the RNG state during each checkpoint. Note that under torch.compile,\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  global _checkpoint_debug_enabled\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Please write a code to concat two strings in python input_string1 = \u001b[1;35minput\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m  and input_str\nI have written this, but the output I get is \u001b[1m[\u001b[0m‘a', 'b’, ‘c’\u001b[1m]\u001b[0m.\nWhat am i doing wrong and what will be the correct way?\ns1 = “”\nfor char in s2:  \n    if \u001b[1m(\u001b[0m\u001b[1;35mchar.isdigit\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m:\n        \u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0mchar\u001b[1m)\u001b[0m\nThis problem can also be solved by using while loop.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Please write a code to concat two strings in python input_string1 = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">input</span><span style=\"font-weight: bold\">()</span>  and input_str\nI have written this, but the output I get is <span style=\"font-weight: bold\">[</span>‘a', 'b’, ‘c’<span style=\"font-weight: bold\">]</span>.\nWhat am i doing wrong and what will be the correct way?\ns1 = “”\nfor char in s2:  \n    if <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">char.isdigit</span><span style=\"font-weight: bold\">())</span>:\n        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>char<span style=\"font-weight: bold\">)</span>\nThis problem can also be solved by using while loop.\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Time taken for inference: \u001b[1;36m83.97\u001b[0m seconds\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Time taken for inference: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83.97</span> seconds\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"mridul161203/Meta-llama-3-8B-Qlora-quantized-GPTQ\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T23:24:08.095447Z","iopub.execute_input":"2024-04-27T23:24:08.096368Z","iopub.status.idle":"2024-04-27T23:24:10.995834Z","shell.execute_reply.started":"2024-04-27T23:24:08.096331Z","shell.execute_reply":"2024-04-27T23:24:10.994786Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c610706b2ef04f06a594d5d1ebddcce4"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/gaurav021201/Meta-Llama-3-8B-Qlora-GPTQ/commit/2975d2211f1e8e1909087d94356e30712d622b52', commit_message='Upload model', commit_description='', oid='2975d2211f1e8e1909087d94356e30712d622b52', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_dataset(df, split=\"train\"):\n    text_col = []\n    instruction = \"\"\"Write a concise summary of the below input text.\n    Return your response in bullet points which covers the key points of the text.\n    Only provide full sentence responses.\"\"\"  # change instuction according to the task\n    if split == \"train\":\n        for _, row in df.iterrows():\n            input_q = row[\"dialogue\"]\n            output = row[\"summary\"]\n            text = (\n                \"### Instruction: \\n\"\n                + instruction\n                + \"\\n### Input: \\n\"\n                + input_q\n                + \"\\n### Response :\\n\"\n                + output\n            )  # output column in training dataset\n            text_col.append(text)\n        df.loc[:, \"text\"] = text_col\n    else:\n        for _, row in df.iterrows():\n            input_q = row[\"dialogue\"]\n            text = (\n                \"### Instruction: \\n\"\n                + instruction\n                + \"\\n### Input: \\n\"\n                + input_q\n                + \"\\n### Response :\\n\"\n            )  # no output column in test dataset\n            text_col.append(text)\n        df.loc[:, \"text\"] = text_col\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-27T23:36:19.725611Z","iopub.execute_input":"2024-04-27T23:36:19.726547Z","iopub.status.idle":"2024-04-27T23:36:19.737039Z","shell.execute_reply.started":"2024-04-27T23:36:19.726512Z","shell.execute_reply":"2024-04-27T23:36:19.735900Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name = \"knkarthick/dialogsum\"\n\nds = load_dataset(dataset_name)\n# %%\ntrain_ds, test_ds = load_dataset(dataset_name, split=[\"train\", \"test[0:200]\"])\n# %%\nimport pandas as pd\ntrain_df = pd.DataFrame(train_ds)\ntest_df = pd.DataFrame(test_ds)\ntrain_df.head()\nfrom datasets import Dataset\ndataset = Dataset.from_pandas(train_df)\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-04-27T23:36:23.486829Z","iopub.execute_input":"2024-04-27T23:36:23.487207Z","iopub.status.idle":"2024-04-27T23:36:30.378626Z","shell.execute_reply.started":"2024-04-27T23:36:23.487177Z","shell.execute_reply":"2024-04-27T23:36:30.377520Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4318545611a4bc38225f8c163eee3c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06915dd893d14a098a06524fc54e542d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8948166d784d1e9f72e918353d1583"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b44156c33c14799bf0a05398ee9adc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9f356a2a19b45119c0d0ad7b5ec9fd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357787b02d724eea9a842702dc6e8185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4bce1e6857f4f648da6b85d28d4aadb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc1bab186ebf4d4ebc4d4b450855ff0f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46a49c00e5e84afeab545a3a05b8c3fd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"from rich import print\nfrom time import perf_counter\nfrom transformers import GenerationConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n#run inference code\nstart_time = perf_counter()\ntext = test_df[\"dialogue\"][1]\ninputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\ngeneration_config = GenerationConfig(\n    penalty_alpha=0.6, \n    do_sample = True, \n    top_k=5, \n    temperature=0.5, \n    repetition_penalty=1.2,\n    pad_token_id=tokenizer.pad_token_id \n)\noutputs = model.generate(\n    **inputs, max_new_tokens=1200, generation_config=generation_config\n)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nend_time = perf_counter()\noutput_time = end_time - start_time\nprint(f\"Time taken for inference: {output_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T23:47:58.676427Z","iopub.execute_input":"2024-04-27T23:47:58.677527Z","iopub.status.idle":"2024-04-27T23:49:03.049301Z","shell.execute_reply.started":"2024-04-27T23:47:58.677492Z","shell.execute_reply":"2024-04-27T23:49:03.048206Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"#Person1#: Ms. Dawson, I need you to take a dictation for me.\n#Person2#: Yes, sir\u001b[33m...\u001b[0m\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n#Person2#: Yes, sir. Go ahead.\n#Person1#: Attention all staff\u001b[33m...\u001b[0m Effective immediately, all office communications are restricted to email \ncorrespondence and official memos. The use of Instant Message programs by employees during working hours is \nstrictly prohibited.\n#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external \ncommunications?\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside \ncommunications.\n#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n#Person1#: They will just have to change their communication methods. I don't want any - one using Instant \nMessaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n#Person2#: This applies to internal and external communications.\n#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on \nprobation. At second offense, the employee will face termination. Any questions regarding this new policy may be \ndirected to department heads.\n#Person2#: Is that all?\n#Person1#: Yes. Please get this memo typed up and distributed to all employees before \u001b[1;36m4\u001b[0m pm. Thank you.\n#Person2#: You're welcome, sir.\nThe speaker wants people to stop sending instant messages at work because they waste time.\nYes. He says he's sick of wasting so much company money on these IMs.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">#Person1#: Ms. Dawson, I need you to take a dictation for me.\n#Person2#: Yes, sir<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n#Person2#: Yes, sir. Go ahead.\n#Person1#: Attention all staff<span style=\"color: #808000; text-decoration-color: #808000\">...</span> Effective immediately, all office communications are restricted to email \ncorrespondence and official memos. The use of Instant Message programs by employees during working hours is \nstrictly prohibited.\n#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external \ncommunications?\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside \ncommunications.\n#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n#Person1#: They will just have to change their communication methods. I don't want any - one using Instant \nMessaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n#Person2#: This applies to internal and external communications.\n#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on \nprobation. At second offense, the employee will face termination. Any questions regarding this new policy may be \ndirected to department heads.\n#Person2#: Is that all?\n#Person1#: Yes. Please get this memo typed up and distributed to all employees before <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> pm. Thank you.\n#Person2#: You're welcome, sir.\nThe speaker wants people to stop sending instant messages at work because they waste time.\nYes. He says he's sick of wasting so much company money on these IMs.\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Time taken for inference: \u001b[1;36m64.3610571549998\u001b[0m seconds\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Time taken for inference: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64.3610571549998</span> seconds\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"!pip install langchain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install chainlit","metadata":{"execution":{"iopub.status.busy":"2024-04-28T00:27:50.071090Z","iopub.execute_input":"2024-04-28T00:27:50.071510Z","iopub.status.idle":"2024-04-28T00:28:15.719882Z","shell.execute_reply.started":"2024-04-28T00:27:50.071474Z","shell.execute_reply":"2024-04-28T00:28:15.718766Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting chainlit\n  Downloading chainlit-1.0.505-py3-none-any.whl.metadata (5.6 kB)\nCollecting aiofiles<24.0.0,>=23.1.0 (from chainlit)\n  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\nCollecting asyncer<0.0.3,>=0.0.2 (from chainlit)\n  Downloading asyncer-0.0.2-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from chainlit) (8.1.7)\nCollecting dataclasses_json<0.6.0,>=0.5.7 (from chainlit)\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\nCollecting fastapi<0.111.0,>=0.110.1 (from chainlit)\n  Downloading fastapi-0.110.2-py3-none-any.whl.metadata (24 kB)\nCollecting fastapi-socketio<0.0.11,>=0.0.10 (from chainlit)\n  Downloading fastapi_socketio-0.0.10-py3-none-any.whl.metadata (2.6 kB)\nCollecting filetype<2.0.0,>=1.2.0 (from chainlit)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from chainlit) (0.27.0)\nCollecting lazify<0.5.0,>=0.4.0 (from chainlit)\n  Downloading Lazify-0.4.0-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting literalai==0.0.507 (from chainlit)\n  Downloading literalai-0.0.507.tar.gz (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from chainlit) (1.5.8)\nRequirement already satisfied: packaging<24.0,>=23.1 in /opt/conda/lib/python3.10/site-packages (from chainlit) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from chainlit) (2.5.3)\nRequirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from chainlit) (2.8.0)\nRequirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from chainlit) (1.0.0)\nCollecting python-graphql-client<0.5.0,>=0.4.3 (from chainlit)\n  Downloading python_graphql_client-0.4.3-py3-none-any.whl.metadata (4.4 kB)\nCollecting python-multipart<0.0.10,>=0.0.9 (from chainlit)\n  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\nCollecting starlette<0.38.0,>=0.37.2 (from chainlit)\n  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\nCollecting syncer<3.0.0,>=2.0.3 (from chainlit)\n  Downloading syncer-2.0.3.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tomli<3.0.0,>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from chainlit) (2.0.1)\nCollecting uptrace<2.0.0,>=1.22.0 (from chainlit)\n  Downloading uptrace-1.24.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: uvicorn<0.26.0,>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from chainlit) (0.25.0)\nCollecting watchfiles<0.21.0,>=0.20.0 (from chainlit)\n  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting chevron>=0.14.0 (from literalai==0.0.507->chainlit)\n  Downloading chevron-0.14.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting anyio<4.0.0,>=3.4.0 (from asyncer<0.0.3,>=0.0.2->chainlit)\n  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses_json<0.6.0,>=0.5.7->chainlit) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses_json<0.6.0,>=0.5.7->chainlit) (0.9.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.111.0,>=0.110.1->chainlit) (4.9.0)\nCollecting python-socketio>=4.6.0 (from fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n  Downloading python_socketio-5.11.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->chainlit) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->chainlit) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->chainlit) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->chainlit) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->chainlit) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->chainlit) (2.14.6)\nRequirement already satisfied: aiohttp~=3.0 in /opt/conda/lib/python3.10/site-packages (from python-graphql-client<0.5.0,>=0.4.3->chainlit) (3.9.1)\nRequirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from python-graphql-client<0.5.0,>=0.4.3->chainlit) (2.31.0)\nRequirement already satisfied: websockets>=5.0 in /opt/conda/lib/python3.10/site-packages (from python-graphql-client<0.5.0,>=0.4.3->chainlit) (12.0)\nCollecting opentelemetry-api~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_api-1.24.0-py3-none-any.whl.metadata (1.3 kB)\nCollecting opentelemetry-exporter-otlp~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_exporter_otlp-1.24.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-instrumentation~=0.45b0 (from uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-sdk~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (4.0.3)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.4.0->asyncer<0.0.3,>=0.0.2->chainlit) (1.2.0)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (1.2.14)\nRequirement already satisfied: importlib-metadata<=7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (6.11.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.24.0 (from opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-exporter-otlp-proto-http==1.24.0 (from opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (1.62.0)\nRequirement already satisfied: grpcio<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (1.51.1)\nCollecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\nCollecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: protobuf<5.0,>=3.19 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-proto==1.24.0->opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (3.20.3)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation~=0.45b0->uptrace<2.0.0,>=1.22.0->chainlit) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation~=0.45b0->uptrace<2.0.0,>=1.22.0->chainlit) (1.14.1)\nCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-sdk~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting bidict>=0.21.0 (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\nCollecting python-engineio>=4.8.0 (from python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n  Downloading python_engineio-4.9.0-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->python-graphql-client<0.5.0,>=0.4.3->chainlit) (1.26.18)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json<0.6.0,>=0.5.7->chainlit) (1.0.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (3.17.0)\nCollecting simple-websocket>=0.10.0 (from python-engineio>=4.8.0->python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n  Downloading simple_websocket-1.0.0-py3-none-any.whl.metadata (1.3 kB)\nCollecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio>=4.6.0->fastapi-socketio<0.0.11,>=0.0.10->chainlit)\n  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\nDownloading chainlit-1.0.505-py3-none-any.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\nDownloading asyncer-0.0.2-py3-none-any.whl (8.3 kB)\nDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nDownloading fastapi-0.110.2-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi_socketio-0.0.10-py3-none-any.whl (7.4 kB)\nDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading Lazify-0.4.0-py2.py3-none-any.whl (3.1 kB)\nDownloading python_graphql_client-0.4.3-py3-none-any.whl (4.9 kB)\nDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\nDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uptrace-1.24.0-py3-none-any.whl (8.6 kB)\nDownloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chevron-0.14.0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp-1.24.0-py3-none-any.whl (7.0 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\nDownloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\nDownloading python_socketio-5.11.2-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\nDownloading python_engineio-4.9.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading simple_websocket-1.0.0-py3-none-any.whl (13 kB)\nDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: literalai, syncer\n  Building wheel for literalai (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for literalai: filename=literalai-0.0.507-py3-none-any.whl size=58377 sha256=fb2b2b511a4121db446b0e4ca98db2832f227e693af161031bce57ff0423951d\n  Stored in directory: /root/.cache/pip/wheels/45/cb/55/a3f35d4a7abc1cf3ec573647e91d7d4787c76dda56a0419327\n  Building wheel for syncer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for syncer: filename=syncer-2.0.3-py2.py3-none-any.whl size=3436 sha256=9cc717c6a7188a558df4b6758b4cd0e0736e160a53fa24c46c451917bdc9be1d\n  Stored in directory: /root/.cache/pip/wheels/09/e4/36/bcaad665bcc2b672888ff2df1a5a1dc638378d8765055313cd\nSuccessfully built literalai syncer\nInstalling collected packages: syncer, lazify, filetype, chevron, wsproto, python-multipart, opentelemetry-semantic-conventions, opentelemetry-proto, bidict, anyio, aiofiles, watchfiles, starlette, simple-websocket, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, dataclasses_json, asyncer, python-graphql-client, python-engineio, opentelemetry-sdk, opentelemetry-instrumentation, literalai, fastapi, python-socketio, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, fastapi-socketio, uptrace, chainlit\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.43b0\n    Uninstalling opentelemetry-semantic-conventions-0.43b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.43b0\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.22.0\n    Uninstalling opentelemetry-proto-1.22.0:\n      Successfully uninstalled opentelemetry-proto-1.22.0\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.2.0\n    Uninstalling anyio-4.2.0:\n      Successfully uninstalled anyio-4.2.0\n  Attempting uninstall: aiofiles\n    Found existing installation: aiofiles 22.1.0\n    Uninstalling aiofiles-22.1.0:\n      Successfully uninstalled aiofiles-22.1.0\n  Attempting uninstall: watchfiles\n    Found existing installation: watchfiles 0.21.0\n    Uninstalling watchfiles-0.21.0:\n      Successfully uninstalled watchfiles-0.21.0\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.32.0.post1\n    Uninstalling starlette-0.32.0.post1:\n      Successfully uninstalled starlette-0.32.0.post1\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.22.0\n    Uninstalling opentelemetry-exporter-otlp-proto-common-1.22.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.22.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.22.0\n    Uninstalling opentelemetry-api-1.22.0:\n      Successfully uninstalled opentelemetry-api-1.22.0\n  Attempting uninstall: dataclasses_json\n    Found existing installation: dataclasses-json 0.6.4\n    Uninstalling dataclasses-json-0.6.4:\n      Successfully uninstalled dataclasses-json-0.6.4\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.22.0\n    Uninstalling opentelemetry-sdk-1.22.0:\n      Successfully uninstalled opentelemetry-sdk-1.22.0\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.108.0\n    Uninstalling fastapi-0.108.0:\n      Successfully uninstalled fastapi-0.108.0\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-http\n    Found existing installation: opentelemetry-exporter-otlp-proto-http 1.22.0\n    Uninstalling opentelemetry-exporter-otlp-proto-http-1.22.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-http-1.22.0\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\n    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.22.0\n    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.22.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.22.0\n  Attempting uninstall: opentelemetry-exporter-otlp\n    Found existing installation: opentelemetry-exporter-otlp 1.22.0\n    Uninstalling opentelemetry-exporter-otlp-1.22.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-1.22.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiofiles-23.2.1 anyio-3.7.1 asyncer-0.0.2 bidict-0.23.1 chainlit-1.0.505 chevron-0.14.0 dataclasses_json-0.5.14 fastapi-0.110.2 fastapi-socketio-0.0.10 filetype-1.2.0 lazify-0.4.0 literalai-0.0.507 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-exporter-otlp-proto-http-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 python-engineio-4.9.0 python-graphql-client-0.4.3 python-multipart-0.0.9 python-socketio-5.11.2 simple-websocket-1.0.0 starlette-0.37.2 syncer-2.0.3 uptrace-1.24.0 watchfiles-0.20.0 wsproto-1.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile app.py\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferWindowMemory\nimport chainlit as cl\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GPTQConfig\n\nmodel_id = \"mridul161203/Meta-llama-3-8B-Qlora-quantized-GPTQ\"\nquantization_config_loading = GPTQConfig(bits=4, disable_exllama=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config_loading, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\npipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512\n    )\n\nllm=HuggingFacePipeline(pipeline=pipe)\n\ntemplate = \"\"\"\n<|system|>\nYou are a helpful assistant that provides information and engages in casual conversation.\nRespond naturally to user queries and provide useful information.\nPlease, write a single reply only!\n</s>\n<|user|>\nCurrent conversation:\n{history}\nQuestion: {input}\n</s>\n<|assistant|>\n\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\nmemory=ConversationBufferWindowMemory(k=3)\n\n@cl.on_chat_start\nasync def start():\n    llm_chain = ConversationChain(prompt=prompt, llm=llm, memory=memory)\n    cl.user_session.set(\"llm_chain\", llm_chain)\n\n\n@cl.on_message\nasync def main(message:cl.message):\n    llm_chain = cl.user_session.get(\"llm_chain\")\n    cb = cl.AsyncLangchainCallbackHandler( )\n    cb.answer_reached = True\n    res = await cl.make_async(llm_chain)(message.content, callbacks=[cb])\n    await cl.Message(content=res['response']).send()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T00:28:21.937294Z","iopub.execute_input":"2024-04-28T00:28:21.937694Z","iopub.status.idle":"2024-04-28T00:28:21.945869Z","shell.execute_reply.started":"2024-04-28T00:28:21.937660Z","shell.execute_reply":"2024-04-28T00:28:21.944824Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-28T00:05:43.567977Z","iopub.execute_input":"2024-04-28T00:05:43.568342Z","iopub.status.idle":"2024-04-28T00:05:44.174221Z","shell.execute_reply.started":"2024-04-28T00:05:43.568314Z","shell.execute_reply":"2024-04-28T00:05:44.173159Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"!pip3 install pyngrok ","metadata":{"execution":{"iopub.status.busy":"2024-04-28T00:26:44.487834Z","iopub.execute_input":"2024-04-28T00:26:44.488213Z","iopub.status.idle":"2024-04-28T00:26:57.299941Z","shell.execute_reply.started":"2024-04-28T00:26:44.488183Z","shell.execute_reply":"2024-04-28T00:26:57.298991Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.1.6-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.1)\nDownloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.1.6\n","output_type":"stream"}]},{"cell_type":"code","source":"!ngrok config add-authtoken 2euAA6pePk1rL9Rhxfey604VGP4_Ka5XQBCSUymbNnnEvout","metadata":{"execution":{"iopub.status.busy":"2024-04-28T00:27:08.124257Z","iopub.execute_input":"2024-04-28T00:27:08.125078Z","iopub.status.idle":"2024-04-28T00:27:09.930402Z","shell.execute_reply.started":"2024-04-28T00:27:08.125039Z","shell.execute_reply":"2024-04-28T00:27:09.929531Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}]},{"cell_type":"code","source":"from pyngrok import ngrok\nprint(ngrok.connect(8000).public_url)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T00:28:31.286727Z","iopub.execute_input":"2024-04-28T00:28:31.287082Z","iopub.status.idle":"2024-04-28T00:28:31.717538Z","shell.execute_reply.started":"2024-04-28T00:28:31.287053Z","shell.execute_reply":"2024-04-28T00:28:31.716593Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"https://aa06-34-72-27-113.ngrok-free.app\n","output_type":"stream"}]},{"cell_type":"code","source":"!chainlit run app.py","metadata":{"execution":{"iopub.status.busy":"2024-04-28T00:28:36.322259Z","iopub.execute_input":"2024-04-28T00:28:36.322643Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/langchain/llms/__init__.py:548: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.llms import HuggingFacePipeline`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n2024-04-28 00:28:47.485186: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 00:28:47.485290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 00:28:47.608984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\nconfig.json: 100%|█████████████████████████| 1.30k/1.30k [00:00<00:00, 8.61MB/s]\n/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n  warnings.warn(warning_msg)\nmodel.safetensors.index.json: 100%|████████| 96.2k/96.2k [00:00<00:00, 2.90MB/s]\nDownloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/4.69G [00:00<?, ?B/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 10.5M/4.69G [00:00<00:54, 85.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|     | 31.5M/4.69G [00:00<00:34, 137MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|     | 62.9M/4.69G [00:00<00:24, 189MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|     | 94.4M/4.69G [00:00<00:21, 215MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏     | 126M/4.69G [00:00<00:19, 233MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏     | 157M/4.69G [00:00<00:18, 244MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏     | 189M/4.69G [00:00<00:17, 251MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎     | 220M/4.69G [00:00<00:17, 254MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎     | 252M/4.69G [00:01<00:17, 255MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎     | 283M/4.69G [00:01<00:20, 215MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▍     | 315M/4.69G [00:01<00:19, 229MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▍     | 346M/4.69G [00:01<00:18, 239MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍     | 377M/4.69G [00:01<00:17, 245MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▌     | 409M/4.69G [00:01<00:17, 251MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▌     | 440M/4.69G [00:01<00:16, 254MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌     | 472M/4.69G [00:02<00:16, 257MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▋     | 503M/4.69G [00:02<00:16, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▋     | 535M/4.69G [00:02<00:15, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▋     | 566M/4.69G [00:02<00:15, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▊     | 598M/4.69G [00:02<00:15, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▊     | 629M/4.69G [00:02<00:15, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▊     | 661M/4.69G [00:02<00:15, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▉     | 692M/4.69G [00:02<00:15, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▉     | 724M/4.69G [00:02<00:15, 259MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▉     | 755M/4.69G [00:03<00:15, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|█     | 786M/4.69G [00:03<00:14, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|█     | 818M/4.69G [00:03<00:14, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|█     | 849M/4.69G [00:03<00:14, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|█▏    | 881M/4.69G [00:03<00:14, 264MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|█▏    | 912M/4.69G [00:03<00:14, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|█▏    | 944M/4.69G [00:03<00:14, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█▏    | 975M/4.69G [00:03<00:14, 264MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 1.01G/4.69G [00:04<00:13, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 1.04G/4.69G [00:04<00:13, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 1.07G/4.69G [00:04<00:13, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 1.10G/4.69G [00:04<00:13, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 1.13G/4.69G [00:04<00:13, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▏   | 1.16G/4.69G [00:04<00:13, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█▎   | 1.20G/4.69G [00:04<00:13, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█▎   | 1.23G/4.69G [00:04<00:13, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█▎   | 1.26G/4.69G [00:04<00:13, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▍   | 1.29G/4.69G [00:05<00:12, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▍   | 1.32G/4.69G [00:05<00:12, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▍   | 1.35G/4.69G [00:05<00:12, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▍   | 1.38G/4.69G [00:05<00:12, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▌   | 1.42G/4.69G [00:05<00:12, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▌   | 1.45G/4.69G [00:05<00:12, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▌   | 1.48G/4.69G [00:05<00:12, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▌   | 1.51G/4.69G [00:05<00:11, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▋   | 1.54G/4.69G [00:06<00:11, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▋   | 1.57G/4.69G [00:06<00:11, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▋   | 1.60G/4.69G [00:06<00:11, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▋   | 1.64G/4.69G [00:06<00:11, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 1.67G/4.69G [00:06<00:11, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 1.70G/4.69G [00:06<00:11, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▊   | 1.73G/4.69G [00:06<00:11, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▉   | 1.76G/4.69G [00:06<00:11, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▉   | 1.79G/4.69G [00:07<00:10, 264MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▉   | 1.82G/4.69G [00:07<00:10, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▉   | 1.86G/4.69G [00:07<00:10, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|██   | 1.89G/4.69G [00:07<00:10, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|██   | 1.92G/4.69G [00:07<00:10, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 1.95G/4.69G [00:07<00:10, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 1.98G/4.69G [00:07<00:10, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|██▏  | 2.01G/4.69G [00:07<00:10, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▏  | 2.04G/4.69G [00:07<00:10, 264MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▏  | 2.08G/4.69G [00:08<00:09, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|██▏  | 2.11G/4.69G [00:08<00:10, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|██▎  | 2.14G/4.69G [00:08<00:09, 257MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|██▎  | 2.17G/4.69G [00:08<00:09, 253MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|██▎  | 2.20G/4.69G [00:08<00:09, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▍  | 2.23G/4.69G [00:08<00:09, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▍  | 2.26G/4.69G [00:08<00:09, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|██▍  | 2.30G/4.69G [00:08<00:09, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██▍  | 2.33G/4.69G [00:09<00:09, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██▌  | 2.36G/4.69G [00:09<00:09, 257MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██▌  | 2.39G/4.69G [00:09<00:08, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██▌  | 2.42G/4.69G [00:09<00:08, 253MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██▌  | 2.45G/4.69G [00:09<00:10, 219MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 2.49G/4.69G [00:09<00:09, 231MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▋  | 2.52G/4.69G [00:09<00:09, 238MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▋  | 2.55G/4.69G [00:09<00:08, 245MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▊  | 2.58G/4.69G [00:10<00:08, 251MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▊  | 2.61G/4.69G [00:10<00:08, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▊  | 2.64G/4.69G [00:10<00:07, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▊  | 2.67G/4.69G [00:10<00:07, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 2.71G/4.69G [00:10<00:07, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 2.74G/4.69G [00:10<00:07, 269MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▉  | 2.77G/4.69G [00:10<00:07, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▉  | 2.80G/4.69G [00:10<00:06, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|███  | 2.83G/4.69G [00:11<00:08, 222MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|███  | 2.86G/4.69G [00:11<00:14, 125MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|██▍ | 2.88G/4.69G [00:12<00:19, 92.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|██▍ | 2.90G/4.69G [00:12<00:21, 81.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|██▍ | 2.93G/4.69G [00:12<00:22, 77.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.94G/4.69G [00:12<00:22, 76.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.95G/4.69G [00:13<00:23, 73.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.96G/4.69G [00:13<00:25, 67.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|██▌ | 2.97G/4.69G [00:13<00:26, 65.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 2.98G/4.69G [00:13<00:26, 65.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 2.99G/4.69G [00:13<00:27, 61.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 3.00G/4.69G [00:13<00:25, 65.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 3.01G/4.69G [00:14<00:27, 60.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|██▌ | 3.02G/4.69G [00:14<00:26, 61.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 3.03G/4.69G [00:14<00:28, 58.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 3.04G/4.69G [00:14<00:28, 57.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 3.05G/4.69G [00:14<00:32, 50.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|██▌ | 3.06G/4.69G [00:15<00:29, 55.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▌ | 3.07G/4.69G [00:15<00:27, 59.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▋ | 3.08G/4.69G [00:15<00:28, 56.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▋ | 3.09G/4.69G [00:15<00:26, 59.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▋ | 3.10G/4.69G [00:15<00:28, 55.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|██▋ | 3.11G/4.69G [00:16<00:29, 53.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|██▋ | 3.12G/4.69G [00:16<00:25, 60.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|██▋ | 3.15G/4.69G [00:16<00:23, 65.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|██▋ | 3.17G/4.69G [00:16<00:17, 87.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|███▍ | 3.19G/4.69G [00:16<00:13, 111MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|███▍ | 3.21G/4.69G [00:16<00:11, 130MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 3.23G/4.69G [00:16<00:11, 130MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 3.25G/4.69G [00:17<00:13, 109MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|██▊ | 3.27G/4.69G [00:17<00:16, 85.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|██▊ | 3.29G/4.69G [00:17<00:19, 72.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|██▊ | 3.31G/4.69G [00:18<00:18, 74.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|██▊ | 3.32G/4.69G [00:18<00:19, 70.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|██▊ | 3.33G/4.69G [00:18<00:19, 69.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|██▊ | 3.34G/4.69G [00:18<00:21, 63.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|██▊ | 3.37G/4.69G [00:18<00:15, 85.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|██▉ | 3.40G/4.69G [00:19<00:13, 95.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|██▉ | 3.41G/4.69G [00:19<00:13, 96.6MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|███▋ | 3.43G/4.69G [00:19<00:10, 115MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|███▋ | 3.46G/4.69G [00:19<00:08, 148MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███▋ | 3.49G/4.69G [00:19<00:06, 172MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███▊ | 3.52G/4.69G [00:19<00:06, 182MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███▊ | 3.54G/4.69G [00:20<00:08, 131MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███▊ | 3.58G/4.69G [00:20<00:07, 156MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 3.61G/4.69G [00:20<00:05, 180MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 3.63G/4.69G [00:20<00:05, 176MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███▉ | 3.65G/4.69G [00:20<00:08, 129MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▉ | 3.68G/4.69G [00:20<00:06, 157MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▉ | 3.71G/4.69G [00:20<00:05, 182MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▉ | 3.74G/4.69G [00:21<00:04, 191MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|████ | 3.77G/4.69G [00:21<00:06, 143MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|████ | 3.81G/4.69G [00:21<00:05, 164MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|████ | 3.84G/4.69G [00:21<00:04, 185MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|████▏| 3.87G/4.69G [00:22<00:05, 140MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|████▏| 3.89G/4.69G [00:22<00:05, 135MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|████▏| 3.92G/4.69G [00:22<00:05, 151MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|████▏| 3.94G/4.69G [00:22<00:06, 109MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|████▏| 3.97G/4.69G [00:22<00:05, 133MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|████▎| 4.01G/4.69G [00:23<00:04, 156MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|████▎| 4.04G/4.69G [00:23<00:03, 179MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  87%|████▎| 4.07G/4.69G [00:23<00:03, 193MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 4.10G/4.69G [00:23<00:02, 210MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 4.13G/4.69G [00:23<00:04, 130MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|████▍| 4.16G/4.69G [00:23<00:03, 154MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|████▍| 4.19G/4.69G [00:24<00:02, 176MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|████▌| 4.23G/4.69G [00:24<00:02, 195MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|████▌| 4.26G/4.69G [00:24<00:02, 213MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|████▌| 4.29G/4.69G [00:24<00:01, 226MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|████▌| 4.32G/4.69G [00:24<00:01, 237MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|████▋| 4.35G/4.69G [00:24<00:01, 203MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|████▋| 4.38G/4.69G [00:25<00:02, 139MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|████▋| 4.41G/4.69G [00:25<00:01, 161MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|████▋| 4.45G/4.69G [00:25<00:01, 182MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|████▊| 4.48G/4.69G [00:25<00:01, 199MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|████▊| 4.51G/4.69G [00:25<00:00, 211MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|████▊| 4.54G/4.69G [00:25<00:00, 220MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|████▉| 4.57G/4.69G [00:25<00:00, 228MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|████▉| 4.60G/4.69G [00:26<00:00, 233MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|████▉| 4.63G/4.69G [00:26<00:00, 196MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|█████| 4.69G/4.69G [00:26<00:00, 177MB/s]\u001b[A\nDownloading shards:  50%|████████████▌            | 1/2 [00:26<00:26, 26.64s/it]\nmodel-00002-of-00002.safetensors:   0%|             | 0.00/1.05G [00:00<?, ?B/s]\u001b[A\nmodel-00002-of-00002.safetensors:   1%|    | 10.5M/1.05G [00:00<00:11, 90.2MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   3%|▏    | 31.5M/1.05G [00:00<00:07, 132MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   6%|▎    | 62.9M/1.05G [00:00<00:05, 186MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   9%|▍    | 94.4M/1.05G [00:00<00:04, 211MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  12%|▋     | 126M/1.05G [00:00<00:04, 217MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  15%|▉     | 157M/1.05G [00:00<00:04, 221MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  18%|█     | 189M/1.05G [00:00<00:03, 232MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  21%|█▎    | 220M/1.05G [00:01<00:03, 239MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  24%|█▍    | 252M/1.05G [00:01<00:03, 245MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  27%|█▌    | 283M/1.05G [00:01<00:03, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  30%|█▊    | 315M/1.05G [00:01<00:02, 251MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  33%|█▉    | 346M/1.05G [00:01<00:02, 255MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  36%|██▏   | 377M/1.05G [00:01<00:02, 251MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  39%|██▎   | 409M/1.05G [00:01<00:02, 255MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  42%|██▌   | 440M/1.05G [00:01<00:02, 254MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  45%|██▋   | 472M/1.05G [00:02<00:02, 246MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  48%|██▊   | 503M/1.05G [00:02<00:02, 246MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  51%|███   | 535M/1.05G [00:02<00:02, 239MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  54%|███▏  | 566M/1.05G [00:02<00:02, 240MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  57%|███▍  | 598M/1.05G [00:02<00:01, 244MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  60%|███▌  | 629M/1.05G [00:02<00:01, 246MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  63%|███▊  | 661M/1.05G [00:02<00:01, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  66%|███▉  | 692M/1.05G [00:02<00:01, 239MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  69%|████▏ | 724M/1.05G [00:03<00:01, 234MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  72%|████▎ | 755M/1.05G [00:03<00:01, 238MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  75%|████▍ | 786M/1.05G [00:03<00:01, 241MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  78%|████▋ | 818M/1.05G [00:03<00:00, 235MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  81%|████▊ | 849M/1.05G [00:03<00:00, 241MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  84%|█████ | 881M/1.05G [00:03<00:00, 247MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  87%|█████▏| 912M/1.05G [00:03<00:00, 251MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  90%|█████▍| 944M/1.05G [00:03<00:00, 251MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  93%|█████▌| 975M/1.05G [00:04<00:00, 252MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  96%|████▊| 1.01G/1.05G [00:04<00:00, 252MB/s]\u001b[A\nmodel-00002-of-00002.safetensors: 100%|█████| 1.05G/1.05G [00:04<00:00, 239MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 2/2 [00:31<00:00, 15.55s/it]\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\nThe cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\nThe sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.29s/it]\ngeneration_config.json: 100%|██████████████████| 172/172 [00:00<00:00, 1.03MB/s]\ntokenizer_config.json: 100%|███████████████| 50.7k/50.7k [00:00<00:00, 13.6MB/s]\ntokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 49.3MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 449/449 [00:00<00:00, 2.79MB/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"}]}]}